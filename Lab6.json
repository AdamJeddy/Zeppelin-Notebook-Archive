{"paragraphs":[{"title":"Starting Hadoop services","text":"%sh\n$HADOOP_HOME/sbin/hadoop-daemon.sh start namenode\n$HADOOP_HOME/sbin/hadoop-daemon.sh start datanode\n\n$HADOOP_HOME/sbin/yarn-daemon.sh start resourcemanager\n$HADOOP_HOME/sbin/yarn-daemon.sh start nodemanager\n\n$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver","user":"anonymous","dateUpdated":"2022-06-17T15:20:58+1000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"starting namenode, logging to /usr/share/hadoop-2.7.3/logs/hadoop-bigdata-namenode-bigdata-VirtualBox.out\nstarting datanode, logging to /usr/share/hadoop-2.7.3/logs/hadoop-bigdata-datanode-bigdata-VirtualBox.out\nstarting resourcemanager, logging to /usr/share/hadoop-2.7.3/logs/yarn-bigdata-resourcemanager-bigdata-VirtualBox.out\nstarting nodemanager, logging to /usr/share/hadoop-2.7.3/logs/yarn-bigdata-nodemanager-bigdata-VirtualBox.out\nstarting historyserver, logging to /usr/share/hadoop-2.7.3/logs/mapred-bigdata-historyserver-bigdata-VirtualBox.out\n"}]},"apps":[],"jobName":"paragraph_1655442134256_-1913120771","id":"20220617-150214_1748808278","dateCreated":"2022-06-17T15:02:14+1000","dateStarted":"2022-06-17T15:20:58+1000","dateFinished":"2022-06-17T15:21:15+1000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:285"},{"text":"%sh\n$HADOOP_HOME/bin/hadoop dfsadmin -safemode leave","user":"anonymous","dateUpdated":"2022-06-17T15:23:10+1000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"DEPRECATED: Use of this script to execute hdfs command is deprecated.\nInstead use the hdfs command for it.\n\nSafe mode is OFF\n"}]},"apps":[],"jobName":"paragraph_1655443355644_-2102568524","id":"20220617-152235_328094026","dateCreated":"2022-06-17T15:22:35+1000","dateStarted":"2022-06-17T15:23:10+1000","dateFinished":"2022-06-17T15:23:14+1000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:286"},{"title":"Start and interact with spark","text":"%spark\nspark","user":"anonymous","dateUpdated":"2022-06-17T15:27:53+1000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"completionKey":"TAB","completionSupport":true,"language":"scala"},"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"ERROR XSDB6: Another instance of Derby may have already booted the database /usr/share/spark-3.0.3-bin-hadoop2.7/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:188)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:358)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:262)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.<init>(HiveExternalCatalog.scala:66)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:166)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:86)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:100)\n\tat org.apache.spark.sql.internal.SessionState.<init>(SessionState.scala:157)\n\tat org.apache.spark.sql.hive.HiveSessionState.<init>(HiveSessionState.scala:32)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:978)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:110)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:109)\n\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$getOrCreate$5.apply(SparkSession.scala:878)\n\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$getOrCreate$5.apply(SparkSession.scala:878)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:878)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter.spark2CreateContext(BaseSparkScalaInterpreter.scala:189)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter.createSparkContext(BaseSparkScalaInterpreter.scala:124)\n\tat org.apache.zeppelin.spark.SparkScala211Interpreter.open(SparkScala211Interpreter.scala:87)\n\tat org.apache.zeppelin.spark.NewSparkInterpreter.open(NewSparkInterpreter.java:102)\n\tat org.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:62)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:617)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:140)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"}]},"apps":[],"jobName":"paragraph_1655442184731_114976468","id":"20220617-150304_1238863743","dateCreated":"2022-06-17T15:03:04+1000","dateStarted":"2022-06-17T15:27:53+1000","dateFinished":"2022-06-17T15:28:00+1000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:287"},{"title":"Create and process DataFrames, and retrieve data from DataFrames","text":"val myRange0 = spark.range(20).toDF(\"number\")\nmyRange0.show()\nval myRange1 = spark.range(18).toDF(“number”)\nmyRange1.show()\nmyRange0.except(myRange1).show()\n","user":"anonymous","dateUpdated":"2022-06-17T15:27:12+1000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"ERROR XSDB6: Another instance of Derby may have already booted the database /usr/share/spark-3.0.3-bin-hadoop2.7/metastore_db.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.privGetJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.getJBMSLockOnDB(Unknown Source)\n\tat org.apache.derby.impl.store.raw.data.BaseDataFileFactory.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore$6.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.raw.RawStore.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.raw.RawStore.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.store.access.RAMAccessManager.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.startModule(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.db.BasicDatabase.bootServiceModule(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.bootStore(Unknown Source)\n\tat org.apache.derby.impl.db.BasicDatabase.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.boot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.TopService.bootModule(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startProviderService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.findProviderAndStartService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$4.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.startPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.bootDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:365)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:394)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:291)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:258)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:76)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:593)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:571)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:624)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:188)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:358)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:262)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.<init>(HiveExternalCatalog.scala:66)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:166)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:86)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:101)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:100)\n\tat org.apache.spark.sql.internal.SessionState.<init>(SessionState.scala:157)\n\tat org.apache.spark.sql.hive.HiveSessionState.<init>(HiveSessionState.scala:32)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:978)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:110)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:109)\n\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$getOrCreate$5.apply(SparkSession.scala:878)\n\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$getOrCreate$5.apply(SparkSession.scala:878)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:99)\n\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:878)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter.spark2CreateContext(BaseSparkScalaInterpreter.scala:189)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter.createSparkContext(BaseSparkScalaInterpreter.scala:124)\n\tat org.apache.zeppelin.spark.SparkScala211Interpreter.open(SparkScala211Interpreter.scala:87)\n\tat org.apache.zeppelin.spark.NewSparkInterpreter.open(NewSparkInterpreter.java:102)\n\tat org.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:62)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:617)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:140)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"}]},"apps":[],"jobName":"paragraph_1655442710162_-2055202852","id":"20220617-151150_1923754166","dateCreated":"2022-06-17T15:11:50+1000","dateStarted":"2022-06-17T15:27:12+1000","dateFinished":"2022-06-17T15:27:22+1000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:288"},{"text":"%sh\necho $SPARK_HOME/README.md","user":"anonymous","dateUpdated":"2022-06-17T15:45:50+1000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"/usr/share/spark-2.1.1/README.md\n"}]},"apps":[],"jobName":"paragraph_1655443632238_-976007043","id":"20220617-152712_1870194116","dateCreated":"2022-06-17T15:27:12+1000","dateStarted":"2022-06-17T15:45:50+1000","dateFinished":"2022-06-17T15:45:50+1000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289"},{"text":"%sh\n$HADOOP_HOME/bin/hadoop fs -put $SPARK_HOME/README.md /user/bigdata/","user":"anonymous","dateUpdated":"2022-06-17T15:46:15+1000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1655444157320_-668921944","id":"20220617-153557_374233593","dateCreated":"2022-06-17T15:35:57+1000","dateStarted":"2022-06-17T15:46:15+1000","dateFinished":"2022-06-17T15:46:21+1000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:290"},{"text":"%sh\n$HADOOP_HOME/bin/hadoop fs -ls /user/bigdata/","user":"anonymous","dateUpdated":"2022-06-17T15:46:44+1000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 14 items\ndrwxr-xr-x   - bigdata supergroup          0 2017-07-03 01:33 /user/bigdata/.sparkStaging\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-29 17:19 /user/bigdata/QuasiMonteCarlo_1651216771928_1927590506\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-29 18:02 /user/bigdata/QuasiMonteCarlo_1651219319803_37587651\n-rw-r--r--   1 bigdata supergroup       3817 2022-06-17 15:46 /user/bigdata/README.md\ndrwxr-xr-x   - bigdata supergroup          0 2022-05-14 23:28 /user/bigdata/a-new-hdfs-folder\ndrwxr-xr-x   - bigdata supergroup          0 2022-06-14 03:15 /user/bigdata/exp-item\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-29 17:22 /user/bigdata/input\n-rw-r--r--   1 bigdata supergroup    4538523 2022-04-29 19:35 /user/bigdata/lab2\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-29 18:55 /user/bigdata/mydir\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-25 00:07 /user/bigdata/myfolder\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-29 18:57 /user/bigdata/output\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-30 22:19 /user/bigdata/output2\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-29 17:57 /user/bigdata/temp\n-rw-r--r--   1 bigdata supergroup    4538523 2022-04-30 22:14 /user/bigdata/text\n"}]},"apps":[],"jobName":"paragraph_1655444289602_-1000212199","id":"20220617-153809_878384800","dateCreated":"2022-06-17T15:38:09+1000","dateStarted":"2022-06-17T15:46:45+1000","dateFinished":"2022-06-17T15:46:49+1000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:291"},{"text":"%sh\n$HADOOP_HOME/bin/hadoop fs -mkdir /user/bigdata/week10","user":"anonymous","dateUpdated":"2022-06-17T15:55:14+1000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1655444804992_-79893172","id":"20220617-154644_368512387","dateCreated":"2022-06-17T15:46:44+1000","dateStarted":"2022-06-17T15:55:14+1000","dateFinished":"2022-06-17T15:55:19+1000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:292"},{"text":"%sh\n$HADOOP_HOME/bin/hadoop fs -ls /user/bigdata/","user":"anonymous","dateUpdated":"2022-06-17T15:55:32+1000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 15 items\ndrwxr-xr-x   - bigdata supergroup          0 2017-07-03 01:33 /user/bigdata/.sparkStaging\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-29 17:19 /user/bigdata/QuasiMonteCarlo_1651216771928_1927590506\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-29 18:02 /user/bigdata/QuasiMonteCarlo_1651219319803_37587651\n-rw-r--r--   1 bigdata supergroup       3817 2022-06-17 15:46 /user/bigdata/README.md\ndrwxr-xr-x   - bigdata supergroup          0 2022-05-14 23:28 /user/bigdata/a-new-hdfs-folder\ndrwxr-xr-x   - bigdata supergroup          0 2022-06-14 03:15 /user/bigdata/exp-item\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-29 17:22 /user/bigdata/input\n-rw-r--r--   1 bigdata supergroup    4538523 2022-04-29 19:35 /user/bigdata/lab2\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-29 18:55 /user/bigdata/mydir\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-25 00:07 /user/bigdata/myfolder\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-29 18:57 /user/bigdata/output\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-30 22:19 /user/bigdata/output2\ndrwxr-xr-x   - bigdata supergroup          0 2022-04-29 17:57 /user/bigdata/temp\n-rw-r--r--   1 bigdata supergroup    4538523 2022-04-30 22:14 /user/bigdata/text\ndrwxr-xr-x   - bigdata supergroup          0 2022-06-17 15:55 /user/bigdata/week10\n"}]},"apps":[],"jobName":"paragraph_1655445314220_956191864","id":"20220617-155514_1502813155","dateCreated":"2022-06-17T15:55:14+1000","dateStarted":"2022-06-17T15:55:32+1000","dateFinished":"2022-06-17T15:55:37+1000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:293"},{"text":"%sh\n$HADOOP_HOME/bin/hadoop fs -put /home/bigdata/Downloads/Resources_week10/people.json /home/bigdata/Downloads/Resources_week10/people.txt /home/bigdata/Downloads/Resources_week10/employees.json /user/bigdata/week10","user":"anonymous","dateUpdated":"2022-06-17T16:02:29+1000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"put: `/user/bigdata/week10/people.json': File exists\nput: `/user/bigdata/week10/employees.json': File exists\n"},{"type":"TEXT","data":"ExitValue: 1"}]},"apps":[],"jobName":"paragraph_1655445332350_-2139631154","id":"20220617-155532_596283639","dateCreated":"2022-06-17T15:55:32+1000","dateStarted":"2022-06-17T16:02:29+1000","dateFinished":"2022-06-17T16:02:35+1000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:294"},{"text":"%sh\n$HADOOP_HOME/bin/hadoop fs -ls /user/bigdata/week10","user":"anonymous","dateUpdated":"2022-06-17T16:02:59+1000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 3 items\n-rw-r--r--   1 bigdata supergroup        130 2022-06-17 16:01 /user/bigdata/week10/employees.json\n-rw-r--r--   1 bigdata supergroup         73 2022-06-17 16:01 /user/bigdata/week10/people.json\n-rw-r--r--   1 bigdata supergroup         32 2022-06-17 16:02 /user/bigdata/week10/people.txt\n"}]},"apps":[],"jobName":"paragraph_1655445487286_-522394267","id":"20220617-155807_890232056","dateCreated":"2022-06-17T15:58:07+1000","dateStarted":"2022-06-17T16:02:59+1000","dateFinished":"2022-06-17T16:03:03+1000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:295"},{"text":"%sh\nscalac -classpath \"$SPARK_HOME/jars/*\" /home/bigdata/Desktop/SimpleApp.scala","user":"anonymous","dateUpdated":"2022-06-17T16:35:30+1000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0\nOpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0\n"}]},"apps":[],"jobName":"paragraph_1655445518918_-1154013632","id":"20220617-155838_1231629079","dateCreated":"2022-06-17T15:58:38+1000","dateStarted":"2022-06-17T16:35:30+1000","dateFinished":"2022-06-17T16:35:42+1000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:296"},{"text":"%sh\njar cvf app.jar SimpleApp*.class","user":"anonymous","dateUpdated":"2022-06-17T16:35:46+1000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"added manifest\nadding: SimpleApp.class(in = 639) (out= 518)(deflated 18%)\nadding: SimpleApp$.class(in = 3933) (out= 1744)(deflated 55%)\n"}]},"apps":[],"jobName":"paragraph_1655447014349_-1506201782","id":"20220617-162334_175666794","dateCreated":"2022-06-17T16:23:34+1000","dateStarted":"2022-06-17T16:35:46+1000","dateFinished":"2022-06-17T16:35:46+1000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:297"},{"text":"%sh\n$SPARK_HOME/bin/spark-submit --master local[*] --class SimpleApp /home/bigdata/Desktop/app.jar","user":"anonymous","dateUpdated":"2022-06-17T16:50:25+1000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"22/06/17 16:50:28 INFO spark.SparkContext: Running Spark version 2.1.1\n22/06/17 16:50:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n22/06/17 16:50:30 INFO spark.SecurityManager: Changing view acls to: bigdata\n22/06/17 16:50:30 INFO spark.SecurityManager: Changing modify acls to: bigdata\n22/06/17 16:50:30 INFO spark.SecurityManager: Changing view acls groups to: \n22/06/17 16:50:30 INFO spark.SecurityManager: Changing modify acls groups to: \n22/06/17 16:50:30 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(bigdata); groups with view permissions: Set(); users  with modify permissions: Set(bigdata); groups with modify permissions: Set()\n22/06/17 16:50:31 INFO util.Utils: Successfully started service 'sparkDriver' on port 44037.\n22/06/17 16:50:31 INFO spark.SparkEnv: Registering MapOutputTracker\n22/06/17 16:50:31 INFO spark.SparkEnv: Registering BlockManagerMaster\n22/06/17 16:50:31 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n22/06/17 16:50:31 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n22/06/17 16:50:31 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-2f3f1dfe-0a24-4718-9647-c0bedf39d943\n22/06/17 16:50:31 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\n22/06/17 16:50:31 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n22/06/17 16:50:31 INFO util.log: Logging initialized @5517ms\n22/06/17 16:50:32 INFO server.Server: jetty-9.2.z-SNAPSHOT\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10c8f62{/jobs/job,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@674c583e{/jobs/job/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/stages,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/stage,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@439a8f59{/stages/pool,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61861a29{/stages/pool/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/storage,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/storage/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/storage/rdd,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage/rdd/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/environment,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/environment/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/executors,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/executors/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/executors/threadDump,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors/threadDump/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/static,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/api,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/jobs/job/kill,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/stages/stage/kill,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 WARN component.AbstractLifeCycle: FAILED Spark@6bab2585{HTTP/1.1}{0.0.0.0:4040}: java.net.BindException: Address already in use\njava.net.BindException: Address already in use\n\tat sun.nio.ch.Net.bind0(Native Method)\n\tat sun.nio.ch.Net.bind(Net.java:461)\n\tat sun.nio.ch.Net.bind(Net.java:453)\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:222)\n\tat sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:85)\n\tat org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)\n\tat org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)\n\tat org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)\n\tat org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)\n\tat org.spark_project.jetty.server.Server.doStart(Server.java:366)\n\tat org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)\n\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:365)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:375)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:375)\n\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2213)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2204)\n\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:375)\n\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:130)\n\tat org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)\n\tat org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:460)\n\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2320)\n\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:868)\n\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:860)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:860)\n\tat SimpleApp$.main(SimpleApp.scala:7)\n\tat SimpleApp.main(SimpleApp.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n22/06/17 16:50:32 WARN component.AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@62923ee6: java.net.BindException: Address already in use\njava.net.BindException: Address already in use\n\tat sun.nio.ch.Net.bind0(Native Method)\n\tat sun.nio.ch.Net.bind(Net.java:461)\n\tat sun.nio.ch.Net.bind(Net.java:453)\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:222)\n\tat sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:85)\n\tat org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)\n\tat org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)\n\tat org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)\n\tat org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)\n\tat org.spark_project.jetty.server.Server.doStart(Server.java:366)\n\tat org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)\n\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:365)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:375)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:375)\n\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2213)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2204)\n\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:375)\n\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:130)\n\tat org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)\n\tat org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:460)\n\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2320)\n\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:868)\n\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:860)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:860)\n\tat SimpleApp$.main(SimpleApp.scala:7)\n\tat SimpleApp.main(SimpleApp.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n22/06/17 16:50:32 INFO server.ServerConnector: Stopped Spark@6bab2585{HTTP/1.1}{0.0.0.0:4040}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@593e824f{/stages/stage/kill,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2f94c4db{/jobs/job/kill,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3954d008{/api,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@591e58fa{/,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6c6357f9{/static,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6fff253c{/executors/threadDump/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4b6579e8{/executors/threadDump,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@62dae245{/executors/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6a1d204a{/executors,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3a4e343{/environment/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@40dd3977{/environment,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@63cd604c{/storage/rdd/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@32cb636e{/storage/rdd,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@25bcd0c7{/storage/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@31024624{/storage,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@61861a29{/stages/pool/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@439a8f59{/stages/pool,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5fb97279{/stages/stage/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5ab14cb9{/stages/stage,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3f23a3a0{/stages/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@25f7391e{/stages,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@674c583e{/jobs/job/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@10c8f62{/jobs/job,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1d9bec4d{/jobs,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n22/06/17 16:50:32 INFO server.Server: jetty-9.2.z-SNAPSHOT\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10c8f62{/jobs/job,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@674c583e{/jobs/job/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/stages,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/stage,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@439a8f59{/stages/pool,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61861a29{/stages/pool/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/storage,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/storage/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/storage/rdd,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage/rdd/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/environment,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/environment/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/executors,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/executors/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/executors/threadDump,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors/threadDump/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/static,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/api,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/jobs/job/kill,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/stages/stage/kill,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 WARN component.AbstractLifeCycle: FAILED Spark@5dcbb60{HTTP/1.1}{0.0.0.0:4041}: java.net.BindException: Address already in use\njava.net.BindException: Address already in use\n\tat sun.nio.ch.Net.bind0(Native Method)\n\tat sun.nio.ch.Net.bind(Net.java:461)\n\tat sun.nio.ch.Net.bind(Net.java:453)\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:222)\n\tat sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:85)\n\tat org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)\n\tat org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)\n\tat org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)\n\tat org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)\n\tat org.spark_project.jetty.server.Server.doStart(Server.java:366)\n\tat org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)\n\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:365)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:375)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:375)\n\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2213)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2204)\n\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:375)\n\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:130)\n\tat org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)\n\tat org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:460)\n\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2320)\n\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:868)\n\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:860)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:860)\n\tat SimpleApp$.main(SimpleApp.scala:7)\n\tat SimpleApp.main(SimpleApp.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n22/06/17 16:50:32 WARN component.AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@21526f6c: java.net.BindException: Address already in use\njava.net.BindException: Address already in use\n\tat sun.nio.ch.Net.bind0(Native Method)\n\tat sun.nio.ch.Net.bind(Net.java:461)\n\tat sun.nio.ch.Net.bind(Net.java:453)\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:222)\n\tat sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:85)\n\tat org.spark_project.jetty.server.ServerConnector.open(ServerConnector.java:321)\n\tat org.spark_project.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)\n\tat org.spark_project.jetty.server.ServerConnector.doStart(ServerConnector.java:236)\n\tat org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)\n\tat org.spark_project.jetty.server.Server.doStart(Server.java:366)\n\tat org.spark_project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68)\n\tat org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:365)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:375)\n\tat org.apache.spark.ui.JettyUtils$$anonfun$5.apply(JettyUtils.scala:375)\n\tat org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:2213)\n\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n\tat org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2204)\n\tat org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:375)\n\tat org.apache.spark.ui.WebUI.bind(WebUI.scala:130)\n\tat org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)\n\tat org.apache.spark.SparkContext$$anonfun$10.apply(SparkContext.scala:460)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:460)\n\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2320)\n\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:868)\n\tat org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:860)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:860)\n\tat SimpleApp$.main(SimpleApp.scala:7)\n\tat SimpleApp.main(SimpleApp.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n22/06/17 16:50:32 INFO server.ServerConnector: Stopped Spark@5dcbb60{HTTP/1.1}{0.0.0.0:4041}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@593e824f{/stages/stage/kill,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2f94c4db{/jobs/job/kill,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3954d008{/api,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@591e58fa{/,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6c6357f9{/static,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6fff253c{/executors/threadDump/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4b6579e8{/executors/threadDump,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@62dae245{/executors/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6a1d204a{/executors,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3a4e343{/environment/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@40dd3977{/environment,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@63cd604c{/storage/rdd/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@32cb636e{/storage/rdd,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@25bcd0c7{/storage/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@31024624{/storage,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@61861a29{/stages/pool/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@439a8f59{/stages/pool,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5fb97279{/stages/stage/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5ab14cb9{/stages/stage,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3f23a3a0{/stages/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@25f7391e{/stages,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@674c583e{/jobs/job/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@10c8f62{/jobs/job,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1d9bec4d{/jobs,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:32 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n22/06/17 16:50:32 INFO server.Server: jetty-9.2.z-SNAPSHOT\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d9bec4d{/jobs,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c48c0c0{/jobs/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10c8f62{/jobs/job,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@674c583e{/jobs/job/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f7391e{/stages,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f23a3a0{/stages/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ab14cb9{/stages/stage,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5fb97279{/stages/stage/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@439a8f59{/stages/pool,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61861a29{/stages/pool/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31024624{/storage,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25bcd0c7{/storage/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32cb636e{/storage/rdd,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63cd604c{/storage/rdd/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dd3977{/environment,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a4e343{/environment/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a1d204a{/executors,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62dae245{/executors/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6579e8{/executors/threadDump,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fff253c{/executors/threadDump/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c6357f9{/static,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591e58fa{/,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3954d008{/api,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f94c4db{/jobs/job/kill,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@593e824f{/stages/stage/kill,null,AVAILABLE,@Spark}\n22/06/17 16:50:32 INFO server.ServerConnector: Started Spark@1551e220{HTTP/1.1}{0.0.0.0:4042}\n22/06/17 16:50:32 INFO server.Server: Started @6360ms\n22/06/17 16:50:32 INFO util.Utils: Successfully started service 'SparkUI' on port 4042.\n22/06/17 16:50:32 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4042\n22/06/17 16:50:32 INFO spark.SparkContext: Added JAR file:/home/bigdata/Desktop/app.jar at spark://10.0.2.15:44037/jars/app.jar with timestamp 1655448632856\n22/06/17 16:50:33 INFO executor.Executor: Starting executor ID driver on host localhost\n22/06/17 16:50:33 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42409.\n22/06/17 16:50:33 INFO netty.NettyBlockTransferService: Server created on 10.0.2.15:42409\n22/06/17 16:50:33 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n22/06/17 16:50:33 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 42409, None)\n22/06/17 16:50:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:42409 with 366.3 MB RAM, BlockManagerId(driver, 10.0.2.15, 42409, None)\n22/06/17 16:50:33 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 42409, None)\n22/06/17 16:50:33 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 42409, None)\n22/06/17 16:50:33 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@76304b46{/metrics/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:33 INFO internal.SharedState: Warehouse path is 'file:/home/bigdata/Desktop/spark-warehouse'.\n22/06/17 16:50:33 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4ee33af7{/SQL,null,AVAILABLE,@Spark}\n22/06/17 16:50:33 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18388a3c{/SQL/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:33 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f212d84{/SQL/execution,null,AVAILABLE,@Spark}\n22/06/17 16:50:33 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b9c69a9{/SQL/execution/json,null,AVAILABLE,@Spark}\n22/06/17 16:50:33 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4dd94a58{/static/sql,null,AVAILABLE,@Spark}\n22/06/17 16:50:43 INFO datasources.FileSourceStrategy: Pruning directories with: \n22/06/17 16:50:43 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n22/06/17 16:50:43 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>\n22/06/17 16:50:43 INFO datasources.FileSourceStrategy: Pushed Filters: \n22/06/17 16:50:46 INFO codegen.CodeGenerator: Code generated in 1090.237814 ms\n22/06/17 16:50:46 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 280.5 KB, free 366.0 MB)\n22/06/17 16:50:46 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KB, free 366.0 MB)\n22/06/17 16:50:46 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:42409 (size: 23.9 KB, free: 366.3 MB)\n22/06/17 16:50:46 INFO spark.SparkContext: Created broadcast 0 from cache at SimpleApp.scala:9\n22/06/17 16:50:46 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n22/06/17 16:50:47 INFO codegen.CodeGenerator: Code generated in 78.014883 ms\n22/06/17 16:50:48 INFO codegen.CodeGenerator: Code generated in 106.816796 ms\n22/06/17 16:50:48 INFO spark.SparkContext: Starting job: count at SimpleApp.scala:10\n22/06/17 16:50:48 INFO scheduler.DAGScheduler: Registering RDD 5 (count at SimpleApp.scala:10)\n22/06/17 16:50:48 INFO scheduler.DAGScheduler: Got job 0 (count at SimpleApp.scala:10) with 1 output partitions\n22/06/17 16:50:48 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (count at SimpleApp.scala:10)\n22/06/17 16:50:48 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n22/06/17 16:50:48 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n22/06/17 16:50:48 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at count at SimpleApp.scala:10), which has no missing parents\n22/06/17 16:50:48 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.2 KB, free 366.0 MB)\n22/06/17 16:50:48 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KB, free 366.0 MB)\n22/06/17 16:50:48 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:42409 (size: 7.5 KB, free: 366.3 MB)\n22/06/17 16:50:48 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:996\n22/06/17 16:50:48 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at count at SimpleApp.scala:10)\n22/06/17 16:50:48 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n22/06/17 16:50:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 6570 bytes)\n22/06/17 16:50:49 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\n22/06/17 16:50:49 INFO executor.Executor: Fetching spark://10.0.2.15:44037/jars/app.jar with timestamp 1655448632856\n22/06/17 16:50:49 INFO client.TransportClientFactory: Successfully created connection to /10.0.2.15:44037 after 103 ms (0 ms spent in bootstraps)\n22/06/17 16:50:49 INFO util.Utils: Fetching spark://10.0.2.15:44037/jars/app.jar to /tmp/spark-fdde429c-ea0e-4c9d-b3c5-ff1c584f4a3c/userFiles-f548c051-301b-4ced-8b78-6933bb187a95/fetchFileTemp3361113301994784720.tmp\n22/06/17 16:50:49 INFO executor.Executor: Adding file:/tmp/spark-fdde429c-ea0e-4c9d-b3c5-ff1c584f4a3c/userFiles-f548c051-301b-4ced-8b78-6933bb187a95/app.jar to class loader\n22/06/17 16:50:49 ERROR executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)\njava.io.IOException: unexpected exception type\n\tat java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1750)\n\tat java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1280)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:479)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:85)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1274)\n\t... 54 more\nCaused by: java.lang.BootstrapMethodError: java.lang.NoClassDefFoundError: scala/runtime/LambdaDeserialize\n\tat SimpleApp$.$deserializeLambda$(SimpleApp.scala)\n\t... 64 more\nCaused by: java.lang.NoClassDefFoundError: scala/runtime/LambdaDeserialize\n\t... 65 more\nCaused by: java.lang.ClassNotFoundException: scala.runtime.LambdaDeserialize\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 65 more\n22/06/17 16:50:49 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.io.IOException: unexpected exception type\n\tat java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1750)\n\tat java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1280)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:479)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:85)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1274)\n\t... 54 more\nCaused by: java.lang.BootstrapMethodError: java.lang.NoClassDefFoundError: scala/runtime/LambdaDeserialize\n\tat SimpleApp$.$deserializeLambda$(SimpleApp.scala)\n\t... 64 more\nCaused by: java.lang.NoClassDefFoundError: scala/runtime/LambdaDeserialize\n\t... 65 more\nCaused by: java.lang.ClassNotFoundException: scala.runtime.LambdaDeserialize\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 65 more\n\n22/06/17 16:50:49 ERROR scheduler.TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\n22/06/17 16:50:49 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n22/06/17 16:50:49 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0\n22/06/17 16:50:49 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (count at SimpleApp.scala:10) failed in 0.999 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.io.IOException: unexpected exception type\n\tat java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1750)\n\tat java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1280)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:479)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:85)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1274)\n\t... 54 more\nCaused by: java.lang.BootstrapMethodError: java.lang.NoClassDefFoundError: scala/runtime/LambdaDeserialize\n\tat SimpleApp$.$deserializeLambda$(SimpleApp.scala)\n\t... 64 more\nCaused by: java.lang.NoClassDefFoundError: scala/runtime/LambdaDeserialize\n\t... 65 more\nCaused by: java.lang.ClassNotFoundException: scala.runtime.LambdaDeserialize\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 65 more\n\nDriver stacktrace:\n22/06/17 16:50:49 INFO scheduler.DAGScheduler: Job 0 failed: count at SimpleApp.scala:10, took 1.467225 s\nException in thread \"main\" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.io.IOException: unexpected exception type\n\tat java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1750)\n\tat java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1280)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:479)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:85)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1274)\n\t... 54 more\nCaused by: java.lang.BootstrapMethodError: java.lang.NoClassDefFoundError: scala/runtime/LambdaDeserialize\n\tat SimpleApp$.$deserializeLambda$(SimpleApp.scala)\n\t... 64 more\nCaused by: java.lang.NoClassDefFoundError: scala/runtime/LambdaDeserialize\n\t... 65 more\nCaused by: java.lang.ClassNotFoundException: scala.runtime.LambdaDeserialize\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 65 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2386)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2385)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2392)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2420)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2419)\n\tat org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2801)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2419)\n\tat SimpleApp$.main(SimpleApp.scala:10)\n\tat SimpleApp.main(SimpleApp.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\nCaused by: java.io.IOException: unexpected exception type\n\tat java.io.ObjectStreamClass.throwMiscException(ObjectStreamClass.java:1750)\n\tat java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1280)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:2093)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1655)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat scala.collection.immutable.List$SerializationProxy.readObject(List.scala:479)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1184)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2296)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2405)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2329)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2187)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:503)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:461)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:85)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1274)\n\t... 54 more\nCaused by: java.lang.BootstrapMethodError: java.lang.NoClassDefFoundError: scala/runtime/LambdaDeserialize\n\tat SimpleApp$.$deserializeLambda$(SimpleApp.scala)\n\t... 64 more\nCaused by: java.lang.NoClassDefFoundError: scala/runtime/LambdaDeserialize\n\t... 65 more\nCaused by: java.lang.ClassNotFoundException: scala.runtime.LambdaDeserialize\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 65 more\n22/06/17 16:50:49 INFO spark.SparkContext: Invoking stop() from shutdown hook\n22/06/17 16:50:49 INFO server.ServerConnector: Stopped Spark@1551e220{HTTP/1.1}{0.0.0.0:4042}\n22/06/17 16:50:49 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@593e824f{/stages/stage/kill,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:49 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2f94c4db{/jobs/job/kill,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:49 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3954d008{/api,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:49 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@591e58fa{/,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:49 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6c6357f9{/static,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:49 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6fff253c{/executors/threadDump/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:49 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4b6579e8{/executors/threadDump,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:49 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@62dae245{/executors/json,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:50 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6a1d204a{/executors,null,UNAVAILABLE,@Spark}\n22/06/17 16:50:50 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3a4e343{/environment/json,null,UNAVAILABLE,@Spark}\n"},{"type":"TEXT","data":"ExitValue: 1"}]},"apps":[],"jobName":"paragraph_1655447746283_-76970193","id":"20220617-163546_1903568389","dateCreated":"2022-06-17T16:35:46+1000","dateStarted":"2022-06-17T16:50:25+1000","dateFinished":"2022-06-17T16:50:50+1000","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:298"},{"text":"%sh\n","user":"anonymous","dateUpdated":"2022-06-17T16:50:25+1000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1655448625723_707457860","id":"20220617-165025_1981482404","dateCreated":"2022-06-17T16:50:25+1000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:299"}],"name":"Lab6","id":"2H89ANTFQ","noteParams":{},"noteForms":{},"angularObjects":{"hbase:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}